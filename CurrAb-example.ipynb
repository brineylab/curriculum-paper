{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "062f587b",
   "metadata": {},
   "source": [
    "## Example Usage of CurrAb\n",
    "\n",
    "Full code used to evaluate models (including inference, per-position inference, and classification) can be found in here: [model-eval](./model-eval/).\n",
    "This notebook only provides a basic example of loading & using CurrAb to predict masked residues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29772a1",
   "metadata": {},
   "source": [
    "### 1. Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaca88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382b4584",
   "metadata": {},
   "source": [
    "### 2. Load model & tokenizer\n",
    "You can find the 650M-parameter models from our paper, including CurrAb, [on Hugging Face](https://huggingface.co/collections/brineylab/curriculum-paper-685b08a4b6986df7c5a5e3c4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c6a18da-8a65-4ba9-a40e-c46d5fc63855",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import EsmTokenizer, EsmForMaskedLM\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# load model & tokenizer\n",
    "model = EsmForMaskedLM.from_pretrained(\"brineylab/CurrAb\").to(device)\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"brineylab/CurrAb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77e2cd",
   "metadata": {},
   "source": [
    "### 3. Format sequences\n",
    "If you want to use our test datasets, you can download them from [Zenodo](https://zenodo.org/records/14661302).\n",
    "\n",
    "For this example, we will use a single paired sequence from the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c86a13bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy_chain = \"EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYDMHWVRQATGKGLEWVSAIGTAGDTYYPGSVKGRFTISRENAKNSLYLQMNSLRAGDTAVYYCARGYCTNGVCYTFGDYGMDVWGQGTTVTVSS\"\n",
    "light_chain = \"DIQMTQSPSSLSASVGDRVTITCRASQSISSYLNWYQQKPGKAPKLLIYAASSLQSGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQSYSTLWTFGQGTKVEIK\"\n",
    "\n",
    "# format sequences with the <cls> separator\n",
    "paired_sequence = f\"{heavy_chain}<cls>{light_chain}\"\n",
    "heavy_sequence = f\"{heavy_chain}<cls>\"\n",
    "light_sequence = f\"<cls>{light_chain}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5003aef9",
   "metadata": {},
   "source": [
    "### 4. Tokenize sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee0af239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 0,  9,  7, 16,  4,  7,  9,  8,  6,  6,  6,  4,  7, 16, 14,  6,  6,  8,\n",
      "          4, 10,  4,  8, 23,  5,  5,  8,  6, 18, 11, 18,  8,  8, 19, 13, 20, 21,\n",
      "         22,  7, 10, 16,  5, 11,  6, 15,  6,  4,  9, 22,  7,  8,  5, 12,  6, 11,\n",
      "          5,  6, 13, 11, 19, 19, 14,  6,  8,  7, 15,  6, 10, 18, 11, 12,  8, 10,\n",
      "          9, 17,  5, 15, 17,  8,  4, 19,  4, 16, 20, 17,  8,  4, 10,  5,  6, 13,\n",
      "         11,  5,  7, 19, 19, 23,  5, 10,  6, 19, 23, 11, 17,  6,  7, 23, 19, 11,\n",
      "         18,  6, 13, 19,  6, 20, 13,  7, 22,  6, 16,  6, 11, 11,  7, 11,  7,  8,\n",
      "          8,  0,  2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "heavy_tokenized = tokenizer(heavy_sequence, return_tensors=\"pt\").to(device)\n",
    "print(heavy_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe0b57-1a84-4574-816f-706adbc93ec3",
   "metadata": {},
   "source": [
    "### 5. run inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd074e7-2289-4b14-b87f-4805ddf2fafc",
   "metadata": {},
   "source": [
    "#### a. Extract logits, attentions, or hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1f9a88f-0a30-4a42-b627-1b3ee0b0e15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([1, 129, 33])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **heavy_tokenized,\n",
    "        output_attentions=False,  # set to True to output attentions\n",
    "        output_hidden_states=False,  # set to True to output hidden states\n",
    "    )\n",
    "logits = outputs.logits\n",
    "print(\"Logits shape:\", logits.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9088e8-c175-4373-a077-7628ee2ffbf4",
   "metadata": {},
   "source": [
    "#### b. Predict a masked position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccd45b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked sequence: EVQLVESGGGLVQPGGSLR<mask>SCAASGFTFSSYDMHWVRQATGKGLEWVSAIGTAGDTYYPGSVKGRFTISRENAKNSLYLQMNSLRAGDTAVYYCARGYCTNGVCYTFGDYGMDVWGQGTTVTVSS<cls>\n"
     ]
    }
   ],
   "source": [
    "# mask the 20th residue (indexing starts at 0)\n",
    "masked_heavy = list(heavy_chain)\n",
    "masked_heavy[19] = \"<mask>\"\n",
    "heavy_masked = \"\".join(masked_heavy) + \"<cls>\"\n",
    "print(\"Masked sequence:\", heavy_masked)\n",
    "\n",
    "# tokenize masked sequence\n",
    "masked_tokenized = tokenizer(heavy_masked, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86883e3-b2e1-4234-89f9-d257dde53d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.2867249846458435\n"
     ]
    }
   ],
   "source": [
    "# run inference with labels\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        **masked_tokenized,\n",
    "        labels=tokenizer(heavy_sequence, return_tensors=\"pt\").input_ids.to(device),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "    )\n",
    "print(\"Loss:\", outputs.loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ca7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted residue: L\n"
     ]
    }
   ],
   "source": [
    "# get predicted token at the masked position\n",
    "mask_token_index = (\n",
    "    (masked_tokenized.input_ids == tokenizer.mask_token_id)\n",
    "    .nonzero(as_tuple=True)[1][0]\n",
    "    .item()\n",
    ")\n",
    "predicted_token_id = outputs.logits[0, mask_token_index].argmax().item()\n",
    "predicted_token = tokenizer.decode([predicted_token_id])\n",
    "print(f\"Predicted residue: {predicted_token}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
